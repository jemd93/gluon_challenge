{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary dependencies\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import nn, rnn\n",
    "import pandas as pd\n",
    "import mxnet.ndarray as F\n",
    "import logging\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "py_new = True #  Checks for python version\n",
    "if sys.version[:1] == '2':\n",
    "    py_new = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check context available and load the necessary context (cpu/gpu)\n",
    "try:\n",
    "    a = mx.nd.ones((2,3), mx.gpu(0))\n",
    "    context = mx.gpu(0)\n",
    "except:\n",
    "    context = mx.cpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # For Python 3.0 and later\n",
    "    from urllib.request import urlopen\n",
    "except ImportError:\n",
    "    # Fall back to Python 2's urllib2\n",
    "    from urllib2 import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAndStoreFile(url):\n",
    "    import os\n",
    "    try:\n",
    "        # For Python 3.0 and later\n",
    "        from urllib.request import urlopen\n",
    "    except ImportError:\n",
    "        # Fall back to Python 2's urllib2\n",
    "        from urllib2 import urlopen\n",
    "    if not os.path.isdir(\"data\"):\n",
    "        response = urlopen(url)\n",
    "        #  There might be problem decoding the response\n",
    "        #  Please checkt he data folder and the file before trying the execrise\n",
    "        #  in windows operating system  'data = response.read()' is enough\n",
    "        data = response.read().decode('UTF-8')\n",
    "        os.mkdir(\"data\")\n",
    "        with open(\"data/nietzsche.txt\", \"w+\") as f:\n",
    "            f.write(data)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\n",
    "readAndStoreFile(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62496\n"
     ]
    }
   ],
   "source": [
    "# loading https://s3.amazonaws.com/text-datasets/nietzsche.txt \n",
    "# You can load anyother text you want\n",
    "# (https://cs.stanford.edu/people/karpathy/char-rnn/)\n",
    "if py_new:\n",
    "    with open(\"data/nietzsche.txt\", errors='ignore') as f:\n",
    "        text = f.read()\n",
    "else:\n",
    "     with open(\"data/nietzsche.txt\") as f:\n",
    "        text = f.read()\n",
    "         \n",
    "df = pd.read_pickle('./data_english.pickle')\n",
    "df_moneymovement = df.loc[df['intent'] == 'i.fai.moneymovement']\n",
    "utterances = df_moneymovement['utterance']\n",
    "\n",
    "text = '\\n'.join(utterances.values)\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 74\n"
     ]
    }
   ],
   "source": [
    "# total of characters in dataset\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)+1\n",
    "print('total chars:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros for padding\n",
    "chars.insert(0, \"\\0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.0123456789?ABCDEFGHIJKLMNOPRSTUVWY]abcdefghijklmnopqrstuvw\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(chars[1:-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps character to unique index e.g. {a:1,b:2....}\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "# maps indices to character (1:a,2:b ....)\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping the dataset into index\n",
    "idx = [char_indices[c] for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62496\n"
     ]
    }
   ],
   "source": [
    "print(len(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wood splitter thing\\nYes unless you can post date the payment\\nyes. i ne'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the mapping\n",
    "''.join(indices_char[i] for i in idx[:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our unrolled RNN\n",
    "\n",
    "In this model we map 3 inputs to one output. Later we will design rnn with n inputs to n inputs (sequence to sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input for neural network(our basic rnn has 3 inputs, n samples)\n",
    "cs = 3\n",
    "c1_dat = [idx[i] for i in range(0, len(idx)-1-cs, cs)]\n",
    "c2_dat = [idx[i+1] for i in range(0, len(idx)-1-cs, cs)]\n",
    "c3_dat = [idx[i+2] for i in range(0, len(idx)-1-cs, cs)]\n",
    "# the output of rnn network (single vector)\n",
    "c4_dat = [idx[i+3] for i in range(0, len(idx)-1-cs, cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking the inputs to form (3 input features )\n",
    "x1 = np.stack(c1_dat[:-2])\n",
    "x2 = np.stack(c2_dat[:-2])\n",
    "x3 = np.stack(c3_dat[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output (1 X N data points)\n",
    "y = np.stack(c4_dat[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20829, 3)\n"
     ]
    }
   ],
   "source": [
    "col_concat = np.array([x1, x2, x3])\n",
    "t_col_concat = col_concat.T\n",
    "print(t_col_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our sample inputs for the model\n",
    "x1_nd = mx.nd.array(x1)\n",
    "x2_nd = mx.nd.array(x2)\n",
    "x3_nd = mx.nd.array(x3)\n",
    "sample_input = mx.nd.array([[x1[0], x2[0], x3[0]], [x1[1], x2[1], x3[1]]])\n",
    "\n",
    "simple_train_data = mx.nd.array(t_col_concat)\n",
    "simple_label_data = mx.nd.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batchsize as 32, so input is of form 32 X 3\n",
    "# output is 32 X 1\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "def get_batch(source, label_data, i, batch_size=32):\n",
    "    bb_size = min(batch_size, source.shape[0] - 1 - i)\n",
    "    data = source[i: i + bb_size]\n",
    "    target = label_data[i: i + bb_size]\n",
    "    # print(target.shape)\n",
    "    return data, target.reshape((-1, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3)\n(32,)\n"
     ]
    }
   ],
   "source": [
    "test_bat, test_target = get_batch(simple_train_data,\n",
    "                                  simple_label_data, 5, batch_size)\n",
    "print(test_bat.shape)\n",
    "print(test_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/unRolled_rnn.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple UnRollredRNN_Model\n",
    "from mxnet.gluon import Block, nn\n",
    "from mxnet import ndarray as F\n",
    "\n",
    "\n",
    "class UnRolledRNN_Model(Block):\n",
    "    def __init__(self, vocab_size, num_embed, num_hidden, **kwargs):\n",
    "        super(UnRolledRNN_Model, self).__init__(**kwargs)\n",
    "        self.num_embed = num_embed\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # use name_scope to give child Blocks appropriate names.\n",
    "        # It also allows sharing Parameters between Blocks recursively.\n",
    "        with self.name_scope():\n",
    "            self.encoder = nn.Embedding(self.vocab_size, self.num_embed)\n",
    "            self.dense1 = nn.Dense(num_hidden, activation='relu', flatten=True)\n",
    "            self.dense2 = nn.Dense(num_hidden, activation='relu', flatten=True)\n",
    "            self.dense3 = nn.Dense(vocab_size, flatten=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        emd = self.encoder(inputs)\n",
    "        # print( emd.shape )\n",
    "        # since the input is shape (batch_size, input(3 characters) )\n",
    "        # we need to extract 0th, 1st, 2nd character from each batch\n",
    "        character1 = emd[:, 0, :]\n",
    "        character2 = emd[:, 1, :]\n",
    "        character3 = emd[:, 2, :]\n",
    "        # green arrow in diagram for character 1\n",
    "        c1_hidden = self.dense1(character1)\n",
    "        # green arrow in diagram for character 2\n",
    "        c2_hidden = self.dense1(character2)\n",
    "        # green arrow in diagram for character 3\n",
    "        c3_hidden = self.dense1(character3)\n",
    "        # yellow arrow in diagram\n",
    "        c1_hidden_2 = self.dense2(c1_hidden)\n",
    "        addition_result = F.add(c2_hidden, c1_hidden_2)  # Total c1 + c2\n",
    "        addition_hidden = self.dense2(addition_result)  # the yellow arrow\n",
    "        addition_result_2 = F.add(addition_hidden, c3_hidden)  # Total c1 + c2\n",
    "        final_output = self.dense3(addition_result_2)\n",
    "        return final_output\n",
    "\n",
    "\n",
    "vocab_size = len(chars) + 1  # the vocabsize\n",
    "num_embed = 30\n",
    "num_hidden = 256\n",
    "# model creation\n",
    "simple_model = UnRolledRNN_Model(vocab_size, num_embed, num_hidden)\n",
    "# model initilisation\n",
    "simple_model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "trainer = gluon.Trainer(simple_model.collect_params(), 'adam')\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "# sample input shape is of size (2x3)\n",
    "# output = simple_model (sample_input)\n",
    "# sample out shape should be (3*87). 87 is our vocab size\n",
    "# print('the output shape',output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory already exists\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# check point file\n",
    "try:\n",
    "    os.makedirs('checkpoints')\n",
    "except:\n",
    "    print(\"directory already exists\")\n",
    "filename_unrolled_rnn = \"checkpoints/rnn_gluon_abc.params\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the actual training\n",
    "def UnRolledRNNtrain(train_data, label_data, batch_size=32, epochs=10):\n",
    "    epochs = epochs\n",
    "    smoothing_constant = .01\n",
    "    for e in range(epochs):\n",
    "        for ibatch, i in enumerate(range(0, train_data.shape[0] - 1, batch_size)):\n",
    "            data, target = get_batch(train_data, label_data, i, batch_size)\n",
    "            data = data.as_in_context(context)\n",
    "            target = target.as_in_context(context)\n",
    "            with autograd.record():\n",
    "                output = simple_model(data)\n",
    "                L = loss(output, target)\n",
    "            L.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "\n",
    "            ##########################\n",
    "            #  Keep a moving average of the losses\n",
    "            ##########################\n",
    "            if ibatch == 128:\n",
    "                curr_loss = mx.nd.mean(L).asscalar()\n",
    "                moving_loss = 0\n",
    "                moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
    "                               else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "                print(\"Epoch %s. Loss: %s, moving_loss %s\" % (e, curr_loss, moving_loss))\n",
    "    simple_model.save_params(filename_unrolled_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 2.3778794, moving_loss 0.023778793811798097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Loss: 1.2496997, moving_loss 0.012496997117996216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2. Loss: 0.97340256, moving_loss 0.009734025597572327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3. Loss: 0.911862, moving_loss 0.009118620157241821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4. Loss: 0.8710758, moving_loss 0.008710758090019225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5. Loss: 0.8234212, moving_loss 0.008234211802482605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6. Loss: 0.7973514, moving_loss 0.007973514199256897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7. Loss: 0.7894658, moving_loss 0.007894657850265503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8. Loss: 0.79595935, moving_loss 0.007959593534469605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9. Loss: 0.7632091, moving_loss 0.007632091045379639\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "UnRolledRNNtrain(simple_train_data, simple_label_data, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model back\n",
    "simple_model.load_params(filename_unrolled_rnn, ctx=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model\n",
    "def evaluate(input_string):\n",
    "    idx = [char_indices[c] for c in input_string]\n",
    "    sample_input = mx.nd.array([[idx[0], idx[1], idx[2]]], ctx=context)\n",
    "    output = simple_model(sample_input)\n",
    "    index = mx.nd.argmax(output, axis=1)\n",
    "    return index.asnumpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the predicted answer is  e\n"
     ]
    }
   ],
   "source": [
    "# predictions\n",
    "begin_char = 'mone'\n",
    "answer = evaluate(begin_char)\n",
    "print('the predicted answer is ', indices_char[answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mx.gluon.rnn.SequentialRNNCell()\n",
    "with model.name_scope():\n",
    "    model.add(mx.gluon.rnn.LSTMCell(20))\n",
    "    model.add(mx.gluon.rnn.LSTMCell(20))\n",
    "states = model.begin_state(batch_size=32)\n",
    "inputs = mx.nd.random.uniform(shape=(5, 32, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character RNN using gluon/lstm api\n",
    "\n",
    "Training sequence 2 sequence models using Gluon API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create model objects.\n",
    "class GluonRNNModel(gluon.Block):\n",
    "    \"\"\"A model with an encoder, recurrent layer, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, mode, vocab_size, num_embed, num_hidden,\n",
    "                 num_layers, dropout=0.5, **kwargs):\n",
    "        super(GluonRNNModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "            self.encoder = nn.Embedding(vocab_size, num_embed,\n",
    "                                        weight_initializer=mx.init.Uniform(0.1))\n",
    "\n",
    "            if mode == 'lstm':\n",
    "                #  we create a LSTM layer with certain number of hidden LSTM cell and layers\n",
    "                #  in our example num_hidden is 1000 and num of layers is 2\n",
    "                #  The input to the LSTM will only be passed during the forward pass (see forward function below)\n",
    "                self.rnn = rnn.LSTM(num_hidden, num_layers, dropout=dropout,\n",
    "                                    input_size=num_embed)\n",
    "            elif mode == 'gru':\n",
    "                #  we create a GRU layer with certain number of hidden GRU cell and layers\n",
    "                #  in our example num_hidden is 1000 and num of layers is 2\n",
    "                #  The input to the GRU will only be passed during the forward pass (see forward function below)\n",
    "                self.rnn = rnn.GRU(num_hidden, num_layers, dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            else:\n",
    "                #  we create a vanilla RNN layer with certain number of hidden vanilla RNN cell and layers\n",
    "                #  in our example num_hidden is 1000 and num of layers is 2\n",
    "                #  The input to the vanilla will only be passed during the forward pass (see forward function below)\n",
    "                self.rnn = rnn.RNN(num_hidden, num_layers, activation='relu', dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            self.decoder = nn.Dense(vocab_size, in_units=num_hidden)\n",
    "            self.num_hidden = num_hidden\n",
    "\n",
    "    #  define the forward pass of the neural network\n",
    "    def forward(self, inputs, hidden):\n",
    "        emb = self.encoder(inputs)\n",
    "        #  emb, hidden are the inputs to the hidden \n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        #  the ouput from the hidden layer to passed to drop out layer\n",
    "        output = self.drop(output)\n",
    "        #  print('output forward',output.shape)\n",
    "        #  Then the output is flattened to a shape for the dense layer  \n",
    "        decoded = self.decoder(output.reshape((-1, self.num_hidden)))\n",
    "        return decoded, hidden\n",
    "\n",
    "    # Initial state of RNN layer\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the lstm\n",
    "mode = 'lstm'\n",
    "# number of characters in vocab_size\n",
    "vocab_size = len(chars) + 1\n",
    "embedsize = 50\n",
    "hididen_units = 1000\n",
    "number_layers = 2\n",
    "clip = 0.2\n",
    "epochs = 200  # use 200 epochs for good result\n",
    "batch_size = 32\n",
    "seq_length = 100  # sequence length\n",
    "dropout = 0.4\n",
    "log_interval = 64\n",
    "# checkpoints/gluonlstm_2 (prepared for seq_lenght 100, 200 epochs)\n",
    "rnn_save = 'checkpoints/gluonlstm_200.params'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GluonRNNModel\n",
    "model = GluonRNNModel(mode, vocab_size, embedsize, hididen_units,\n",
    "                      number_layers, dropout)\n",
    "# initalise the weights of models to random weights\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "# Adam trainer\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam')\n",
    "# softmax cros entropy loss\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepares rnn batches\n",
    "# The batch will be of shape is (num_example * batch_size) because of RNN uses sequences of input     x\n",
    "# for example if we use (a1,a2,a3) as one input sequence , (b1,b2,b3) as another input sequence and (c1,c2,c3)\n",
    "# if we have batch of 3, then at timestep '1'  we only have (a1,b1.c1) as input, at timestep '2' we have (a2,b2,c2) as input...\n",
    "# hence the batchsize is of order \n",
    "# In feedforward we use (batch_size, num_example)\n",
    "def rnn_batch(data, batch_size):\n",
    "    \"\"\"Reshape data into (num_example, batch_size)\"\"\"\n",
    "    nbatch = data.shape[0] // batch_size\n",
    "    data = data[:nbatch * batch_size]\n",
    "    data = data.reshape((batch_size, nbatch)).T\n",
    "    return data\n",
    "\n",
    "idx_nd = mx.nd.array(idx)\n",
    "# convert the idex of characters\n",
    "train_data_rnn_gluon = rnn_batch(idx_nd, batch_size).as_in_context(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the batch\n",
    "def get_batch(source, i, seq):\n",
    "    seq_len = min(seq, source.shape[0] - 1 - i)\n",
    "    data = source[i: i + seq_len]\n",
    "    target = source[i + 1: i + 1 + seq_len]\n",
    "    return data, target.reshape((-1,))\n",
    "\n",
    "\n",
    "# detach the hidden state, so we dont accidentally compute gradients\n",
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [i.detach() for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGluonRNN(epochs, train_data, seq=seq_length):\n",
    "    for epoch in range(epochs):\n",
    "        total_L = 0.0\n",
    "        hidden = model.begin_state(func=mx.nd.zeros, batch_size=batch_size, ctx=context)\n",
    "        for ibatch, i in enumerate(range(0, train_data.shape[0] - 1, seq_length)):\n",
    "            data, target = get_batch(train_data, i, seq)\n",
    "            hidden = detach(hidden)\n",
    "            with autograd.record():\n",
    "                output, hidden = model(data, hidden)\n",
    "                L = loss(output, target) # this is total loss associated with seq_length\n",
    "                L.backward()\n",
    "\n",
    "            grads = [i.grad(context) for i in model.collect_params().values()]\n",
    "            # Here gradient is for the whole batch.\n",
    "            # So we multiply max_norm by batch_size and seq_length to balance it.\n",
    "            gluon.utils.clip_global_norm(grads, clip * seq_length * batch_size)\n",
    "\n",
    "            trainer.step(batch_size)\n",
    "            total_L += mx.nd.sum(L).asscalar()\n",
    "\n",
    "            if ibatch % log_interval == 0 and ibatch > 0:\n",
    "                cur_L = total_L / seq_length / batch_size / log_interval\n",
    "                print('[Epoch %d Batch %d] loss %.2f' % (epoch + 1, ibatch, cur_L))\n",
    "                total_L = 0.0\n",
    "        model.save_params(rnn_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the train data shape is (1953, 32)\n"
     ]
    }
   ],
   "source": [
    "print('the train data shape is', train_data_rnn_gluon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-b6048b60ef7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# The train data shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainGluonRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_rnn_gluon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-35cdd522bf4d>\u001b[0m in \u001b[0;36mtrainGluonRNN\u001b[0;34m(epochs, train_data, seq)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# Here gradient is for the whole batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# So we multiply max_norm by batch_size and seq_length to balance it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_global_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Env/finn-dl/lib/python3.6/site-packages/mxnet/gluon/utils.py\u001b[0m in \u001b[0;36mclip_global_norm\u001b[0;34m(arrays, max_norm)\u001b[0m\n\u001b[1;32m    121\u001b[0m     total_norm = ndarray.add_n(*[ndarray.dot(x, x).as_in_context(ctx)\n\u001b[1;32m    122\u001b[0m                                  for x in (arr.reshape((-1,)) for arr in arrays)])\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         warnings.warn(UserWarning('nan or inf is detected. Clipping results will be undefined.'),\n",
      "\u001b[0;32m~/Env/finn-dl/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1809\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1811\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Env/finn-dl/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1791\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1794\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# The train data shape\n",
    "trainGluonRNN(10, train_data_rnn_gluon, seq=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_params(rnn_save, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates a seqtoseq model over input string\n",
    "def evaluate_seq2seq(model, input_string, seq_length, batch_size):\n",
    "    idx = [char_indices[c] for c in input_string]\n",
    "    if(len(input_string) != seq_length):\n",
    "        raise ValueError(\"input string should be equal to sequence length\")\n",
    "    hidden = model.begin_state(func=mx.nd.zeros, batch_size=batch_size, ctx=context)\n",
    "    sample_input = mx.nd.array(np.array([idx[0:seq_length]]).T, ctx=context)\n",
    "    output, hidden = model(sample_input, hidden)\n",
    "    index = mx.nd.argmax(output, axis=1)\n",
    "    index = index.asnumpy()\n",
    "    return [indices_char[char] for char in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps the input sequence to output sequence\n",
    "def mapInput(input_str, output_str):\n",
    "    for i, _ in enumerate(input_str):\n",
    "        partial_input = input_str[:i+1]\n",
    "        partial_output = output_str[i:i+1]\n",
    "        print(partial_input + \"->\" + partial_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "p->h\n",
      "pr->i\n",
      "pro->b\n",
      "prob->a\n",
      "proba->b\n",
      "probab->l\n",
      "probabl->y\n",
      "probably-> \n",
      "probably ->s\n",
      "probably t->h\n",
      "probably th->i\n",
      "probably the-> \n",
      "probably the ->b\n",
      "probably the t->r\n",
      "probably the ti->m\n",
      "probably the tim->e\n",
      "probably the time-> \n",
      "probably the time ->o\n",
      "probably the time i->s\n",
      "probably the time is-> \n",
      "probably the time is ->a\n",
      "probably the time is a->t\n",
      "probably the time is at-> \n",
      "probably the time is at ->h\n",
      "probably the time is at h->a\n",
      "probably the time is at ha->n\n",
      "probably the time is at han->d\n",
      "probably the time is at hand-> \n",
      "probably the time is at hand ->w\n",
      "probably the time is at hand w->h\n",
      "probably the time is at hand wh->e\n",
      "probably the time is at hand whe->n\n",
      "probably the time is at hand when-> \n",
      "probably the time is at hand when ->i\n",
      "probably the time is at hand when i->t\n",
      "probably the time is at hand when it-> \n",
      "probably the time is at hand when it ->w\n",
      "probably the time is at hand when it w->i\n",
      "probably the time is at hand when it wi->l\n",
      "probably the time is at hand when it wil->l\n",
      "probably the time is at hand when it will-> \n",
      "probably the time is at hand when it will ->b\n",
      "probably the time is at hand when it will b->e\n",
      "probably the time is at hand when it will be-> \n",
      "probably the time is at hand when it will be ->o\n",
      "probably the time is at hand when it will be o->n\n",
      "probably the time is at hand when it will be on->c\n",
      "probably the time is at hand when it will be onc->e\n",
      "probably the time is at hand when it will be once->\n",
      "\n",
      "probably the time is at hand when it will be once ->a\n",
      "probably the time is at hand when it will be once a->n\n",
      "probably the time is at hand when it will be once an->d\n",
      "probably the time is at hand when it will be once and-> \n",
      "probably the time is at hand when it will be once and ->a\n",
      "probably the time is at hand when it will be once and a->g\n",
      "probably the time is at hand when it will be once and ag->a\n",
      "probably the time is at hand when it will be once and aga->i\n",
      "probably the time is at hand when it will be once and agai->n\n",
      "probably the time is at hand when it will be once and again-> \n",
      "probably the time is at hand when it will be once and again ->u\n",
      "probably the time is at hand when it will be once and again u->n\n",
      "probably the time is at hand when it will be once and again un->d\n",
      "probably the time is at hand when it will be once and again und->e\n",
      "probably the time is at hand when it will be once and again unde->r\n",
      "probably the time is at hand when it will be once and again under->s\n",
      "probably the time is at hand when it will be once and again unders->t\n",
      "probably the time is at hand when it will be once and again underst->o\n",
      "probably the time is at hand when it will be once and again understo->o\n",
      "probably the time is at hand when it will be once and again understoo->d\n",
      "probably the time is at hand when it will be once and again understood-> \n",
      "probably the time is at hand when it will be once and again understood ->W\n",
      "probably the time is at hand when it will be once and again understood W->H\n",
      "probably the time is at hand when it will be once and again understood WH->A\n",
      "probably the time is at hand when it will be once and again understood WHA->T\n",
      "probably the time is at hand when it will be once and again understood WHAT-> \n",
      "probably the time is at hand when it will be once and again understood WHAT ->h\n",
      "probably the time is at hand when it will be once and again understood WHAT h->a\n",
      "probably the time is at hand when it will be once and again understood WHAT ha->s\n",
      "probably the time is at hand when it will be once and again understood WHAT has-> \n",
      "probably the time is at hand when it will be once and again understood WHAT has ->a\n",
      "probably the time is at hand when it will be once and again understood WHAT has a->c\n",
      "probably the time is at hand when it will be once and again understood WHAT has ac->t\n",
      "probably the time is at hand when it will be once and again understood WHAT has act->u\n",
      "probably the time is at hand when it will be once and again understood WHAT has actu->a\n",
      "probably the time is at hand when it will be once and again understood WHAT has actua->l\n",
      "probably the time is at hand when it will be once and again understood WHAT has actual->l\n",
      "probably the time is at hand when it will be once and again understood WHAT has actuall->y\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually-> \n",
      "probably the time is at hand when it will be once and again understood WHAT has actually ->s\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually s->u\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually su->f\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually suf->f\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually suff->i\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually suffi->c\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually suffic->e\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually suffice->d\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually sufficed-> \n",
      "probably the time is at hand when it will be once and again understood WHAT has actually sufficed ->f\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually sufficed a->n\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually sufficed an->d\n"
     ]
    }
   ],
   "source": [
    "test_input = 'probably the time is at hand when it will be once and again understood WHAT has actually sufficed an'\n",
    "print(len(test_input))\n",
    "result = evaluate_seq2seq(model, test_input, seq_length, 1)\n",
    "mapInput(test_input, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a nietzsche like text generator\n",
    "import sys\n",
    "\n",
    "\n",
    "def generate_random_text(model, input_string, seq_length, batch_size, sentence_length):\n",
    "    count = 0\n",
    "    new_string = ''\n",
    "    cp_input_string = input_string\n",
    "    hidden = model.begin_state(func=mx.nd.zeros, batch_size=batch_size, ctx=context)\n",
    "    while count < sentence_length:\n",
    "        idx = [char_indices[c] for c in input_string]\n",
    "        if(len(input_string) != seq_length):\n",
    "            print(len(input_string))\n",
    "            raise ValueError('there was a error in the input ')\n",
    "        sample_input = mx.nd.array(np.array([idx[0:seq_length]]).T, ctx=context)\n",
    "        output, hidden = model(sample_input, hidden)\n",
    "        index = mx.nd.argmax(output, axis=1)\n",
    "        index = index.asnumpy()\n",
    "        count = count + 1\n",
    "        new_string = new_string + indices_char[index[-1]]\n",
    "        input_string = input_string[1:] + indices_char[index[-1]]\n",
    "    print(cp_input_string + new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probably the time is at hand when it will be once and again understood WHAT has actually sufficed and finally such a man, things conditions for his task; this task itself demands something\n",
      "else--it requires him TO CREATE VALUES. The philosophical workers, after\n",
      "the excellent pattern of Kant and Hege\n"
     ]
    }
   ],
   "source": [
    "generate_random_text(model, \"probably the time is at hand when it will be once and again understood WHAT has actually sufficed an\", seq_length, 1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
